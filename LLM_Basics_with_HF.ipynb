{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfsDR_omdNea"
   },
   "source": [
    "# LLM Basics with Hugging Face\n",
    "This notebook demonstrates how to load LLM models by utilizing Hugging Face, and how to make queries.\n",
    "<!--table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/Gemma_Basics_with_HF.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Adapted for EECE.4860/5860 at UMass Lowell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaqZItBdeokU"
   },
   "source": [
    "## Prerequisites \n",
    "\n",
    "### Account on Intel Tiber AI Cloud (or run on your local GPU if available)\n",
    "\n",
    "You will need a standard account on Intel Tiber AI Cloud, where we have tested this notebook. Students have been given instructions on how to sign up for an account on Intel Tiber.\n",
    "\n",
    "Once you open this notebook on Tiber cloud, make sure to select **PyTorch GPU** kernel to run it.\n",
    "\n",
    "### HuggingFace setup\n",
    "\n",
    "Before we dive into the tutorial, let's get you set up with HuggingFace:\n",
    "\n",
    "1. **Hugging Face Account:**  If you don't already have one, you can create a free Hugging Face account by clicking [here](https://huggingface.co/join).\n",
    "2. **LLM Model Access:** Head over to the [Gemma model page](https://huggingface.co/google/gemma-2b) and [llama2 model papge](https://huggingface.co/meta-llama/Llama-2-7b-hf) and accept the usage conditions.\n",
    "3. **Hugging Face Token:**  You need to create a token on HuggingFace and use it to login from this notebook. Once you are logged in, you can download the models. Check [this guide](https://huggingface.co/docs/hub/en/security-tokens) on how to create a token on HF. Generate a Hugging Face access (preferably `write` permission) token by clicking [here](https://huggingface.co/settings/tokens). **Save the token in a safe document that you can access**. Once you've completed these steps, you're ready to move on to the next section where we'll install necessary packages and log into HuggingFace Hub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwjo5_Uucxkw"
   },
   "source": [
    "### Import Necessary Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A9sUQ4WrP-Yr"
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwjo5_Uucxkw"
   },
   "source": [
    "### Install dependencies\n",
    "Run the cell below to install all the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r_nXPEsF7UWQ"
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install --upgrade -q transformers huggingface_hub peft \\\n",
    "  accelerate bitsandbytes datasets trl ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_bahJBmwvSp"
   },
   "source": [
    "### Log into Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GIFFCHi-wvSp"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d07333e2384da49ebd53c3ee3fb0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you could use OS env variable to store the HF token\n",
    "#from huggingface_hub import login\n",
    "#login(os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# or use an input box on this notebook to copy/paste the token\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFLddpGeaKh5"
   },
   "source": [
    "**If there is no error in the previous step, you are all set and ready to explore the possibilities with LLM models!**\n",
    "\n",
    "\n",
    "**You need to click the next cell to proceed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXFZFUJHgTcU"
   },
   "source": [
    "## Instantiate the Gemma 2B model (or other models)\n",
    "\n",
    "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
    "\n",
    "Please note we list here a few variants of the Gemma models for you to play with.\n",
    "\n",
    "Other models is this example include Llama 2 from Meta.\n",
    "\n",
    "Let's get started by loading the model from Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgl8ZjHpwvSq"
   },
   "source": [
    "### Loading the model from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w_z4600bwvSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: xpu:0\n"
     ]
    }
   ],
   "source": [
    "#model_id = \"google/gemma-1.1-2b-it\"\n",
    "#model_id = \"google/gemma-2-2b-it\"\n",
    "model_id = \"google/gemma-2-9b-it\"\n",
    "#model_id = \"google/gemma-2-27b-it\"\n",
    "#model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "#device = \"cuda\"\n",
    "USE_CPU = False\n",
    "device = \"xpu:0\" if torch.xpu.is_available() else \"cpu\"\n",
    "if USE_CPU:\n",
    "    device = \"cpu\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "74tpQWWWwvSq"
   },
   "outputs": [],
   "source": [
    "# Let's load the tokenizer first\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UD-eXTxxwvSq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5f87c7f02041cebd1019bd59921972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# We could typically quantize the model to reduce its weight\n",
    "# But to simplify the process, we won't quantize it in this notebook\n",
    "\n",
    "# Let's load the chosen model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyw7fwOGwvSq"
   },
   "source": [
    "### Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nrVBVTtlwvSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favourite color is blue. It's the color of the sky and the ocean, and it always makes me feel\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My favourite color is\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=20)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nrVBVTtlwvSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who won the 2016 baseball World Series? Answer: The Chicago Cubs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who won the 2016 baseball World Series? Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=40)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to judge Umass Lowell? Answer: It depends on what you're looking for.\n",
      "\n",
      "**Here's a breakdown to help you decide:**\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Strong STEM programs:** UMass Lowell excels in engineering, computer science, and other STEM fields. They have state-of-the-art facilities and experienced faculty.\n",
      "* **Affordable tuition:** Compared to other public universities in Massachusetts, UMass Lowell offers relatively affordable tuition.\n",
      "* **Location:** Lowell is a vibrant city with a rich history and a growing economy. It's also conveniently located near Boston and other major cities.\n",
      "* **Research opportunities:** UMass Lowell encourages undergraduate research and offers many opportunities for students to get involved.\n",
      "* **Diverse student body:** UMass Lowell has a diverse student population from all over the world.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "* **Large class sizes:** Some introductory courses can have large class sizes, which may make it harder to get individual attention from professors.\n",
      "* **Limited campus life:** Compared to larger universities, UMass Lowell has a smaller campus and a less vibrant social scene.\n",
      "* **Limited graduate programs:** While UMass Lowell has a strong undergraduate program, its graduate program offerings are more limited.\n",
      "\n",
      "**Overall:**\n",
      "\n",
      "UMass Lowell is a good choice for students who are looking for a high-quality education in STEM fields at an affordable price. It's also a good option for students who want to live in a vibrant city with easy access to Boston. However, students who are looking for a large campus with a vibrant social scene or a wide range of graduate programs may want to consider other options.\n",
      "\n",
      "**To make the best decision for you, consider:**\n",
      "\n",
      "* **Your academic interests:** Does UMass Lowell offer strong programs in your field of study?\n",
      "* **Your budget:** Can you afford the tuition and living expenses at UMass Lowell?\n",
      "* **Your preferred campus environment:** Do you prefer a large or small campus? A bustling or quiet environment?\n",
      "* **Your career goals:** What kind of job do you want after graduation? Will a degree from UMass Lowell help you achieve your goals?\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How to judge Umass Lowell? Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zYT6m2LNvxdo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can you use an LLM for? Answer:\n",
      "\n",
      "LLMs are incredibly versatile and can be used for a wide range of applications, including:\n",
      "\n",
      "**Communication and Language:**\n",
      "\n",
      "* **Chatbots and Conversational AI:** Create interactive chatbots for customer service, education, or entertainment.\n",
      "* **Text Generation:** Generate creative content such as stories, poems, articles, and dialogue.\n",
      "* **Language Translation:** Translate text between languages with high accuracy.\n",
      "* **Summarization and Paraphrasing:** Summarize large amounts of text or rephrase it in different ways.\n",
      "* **Grammar and Spelling Correction:** Improve the quality of written text by correcting errors.\n",
      "\n",
      "**Information Retrieval and Knowledge Management:**\n",
      "\n",
      "* **Question Answering:** Provide answers to questions based on a given context or knowledge base.\n",
      "* **Search Engine Optimization (SEO):** Generate relevant keywords and content for search engines.\n",
      "* **Document Analysis and Classification:** Categorize and analyze documents based on their content.\n",
      "* **Data Extraction:** Extract specific information from unstructured text data.\n",
      "\n",
      "**Code and Software Development:**\n",
      "\n",
      "* **Code Generation:** Generate code in different programming languages.\n",
      "* **Code Completion and Suggestion:** Assist developers by suggesting code snippets and completing code blocks.\n",
      "* **Code Documentation:** Generate documentation for code based on its structure and comments.\n",
      "* **Bug Detection and Repair:** Identify and suggest fixes for potential bugs in code.\n",
      "\n",
      "**Other Applications:**\n",
      "\n",
      "* **Personalized Recommendations:** Recommend products, services, or content based on user preferences.\n",
      "* **Education and Training:** Create interactive learning experiences and provide personalized feedback.\n",
      "* **Healthcare:** Assist with medical diagnosis, patient communication, and drug discovery.\n",
      "* **Research:** Analyze large datasets of text and uncover patterns and insights.\n",
      "\n",
      "This list is not exhaustive, and the potential applications of LLMs are constantly expanding as the technology continues to evolve.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What can you use an LLM for? Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate a list of winners and years of the World Series Championships from 2010 to 2025, the ouput should be the json format. Answer:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"2010\": \"San Francisco Giants\",\n",
      "  \"2011\": \"St. Louis Cardinals\",\n",
      "  \"2012\": \"San Francisco Giants\",\n",
      "  \"2013\": \"Boston Red Sox\",\n",
      "  \"2014\": \"San Francisco Giants\",\n",
      "  \"2015\": \"Kansas City Royals\",\n",
      "  \"2016\": \"Chicago Cubs\",\n",
      "  \"2017\": \"Houston Astros\",\n",
      "  \"2018\": \"Boston Red Sox\",\n",
      "  \"2019\": \"Washington Nationals\",\n",
      "  \"2020\": \"Los Angeles Dodgers\",\n",
      "  \"2021\": \"Atlanta Braves\",\n",
      "  \"2022\": \"Houston Astros\",\n",
      "  \"2023\": null,\n",
      "  \"2024\": null,\n",
      "  \"2025\": null\n",
      "}\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"generate a list of winners and years of the World Series Championships from 2010 to 2025, the ouput should be the json format. Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate a list of winners and years of the World Series Championships from 2010 to 2025, the ouput should be the json format.The json output must include the year of 2024. Answer:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"2010\": \"San Francisco Giants\",\n",
      "  \"2011\": \"St. Louis Cardinals\",\n",
      "  \"2012\": \"San Francisco Giants\",\n",
      "  \"2013\": \"Boston Red Sox\",\n",
      "  \"2014\": \"San Francisco Giants\",\n",
      "  \"2015\": \"Kansas City Royals\",\n",
      "  \"2016\": \"Chicago Cubs\",\n",
      "  \"2017\": \"Houston Astros\",\n",
      "  \"2018\": \"Boston Red Sox\",\n",
      "  \"2019\": \"Washington Nationals\",\n",
      "  \"2020\": \"Los Angeles Dodgers\",\n",
      "  \"2021\": \"Atlanta Braves\",\n",
      "  \"2022\": \"Houston Astros\",\n",
      "  \"2023\": \"TBD\",\n",
      "  \"2024\": \"TBD\",\n",
      "  \"2025\": \"TBD\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Structure:** The JSON object uses the year as the key and the winning team as the value.\n",
      "* **Years:** The years range from 2010 to 2025, including 2024.\n",
      "* **TBD:**  For the years 2023, 2024, and 2025, \"TBD\" is used to indicate that the World Series champion has not yet been determined.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"generate a list of winners and years of the World Series Championships from 2010 to 2025, the ouput should be the json format.The json output must include the year of 2024. Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate a list of winners and years of the World Series Championships from 2010 to 2025, the ouput need to discuss the information about the year 2025. Answer:\n",
      "\n",
      "It's impossible to provide the World Series winner for 2025 because the event hasn't happened yet! \n",
      "\n",
      "Here are the World Series champions from 2010 to 2023:\n",
      "\n",
      "* **2010:** San Francisco Giants\n",
      "* **2011:** St. Louis Cardinals\n",
      "* **2012:** San Francisco Giants\n",
      "* **2013:** Boston Red Sox\n",
      "* **2014:** San Francisco Giants\n",
      "* **2015:** Kansas City Royals\n",
      "* **2016:** Chicago Cubs\n",
      "* **2017:** Houston Astros\n",
      "* **2018:** Boston Red Sox\n",
      "* **2019:** Washington Nationals\n",
      "* **2020:** Los Angeles Dodgers\n",
      "* **2021:** Atlanta Braves\n",
      "* **2022:** Houston Astros\n",
      "* **2023:** Houston Astros \n",
      "\n",
      "\n",
      "We'll have to wait and see who takes home the trophy in 2025! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"generate a list of winners and years of the World Series Championships from 2010 to 2025, the ouput need to discuss the information about the year 2025. Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Gemma_Basics_with_HF.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PyTorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
